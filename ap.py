import pandas as pd
import streamlit as st
from pandas import Series
from sqlalchemy.exc import OperationalError  # Para capturar errores comunes de SQL
import os  # Necesario para obtener la fecha de modificaci√≥n del archivo
from datetime import datetime
try:
    import reverse_geocoder as rg
except ImportError:
    # Si no est√° instalada, rg ser√° None. Lo manejaremos m√°s adelante.
    rg = None


def actualizar_csv_con_st_connection(
        nombre_conexion_st: str,
        consulta_sql: str,
        archivo_csv: str = "data.csv"
):
    """
    Conecta a MySQL usando el metodo moderno st.connection y actualiza un CSV.

    Args:
        nombre_conexion_st (str): El nombre de la conexi√≥n definida en
                                  secrets.toml (ej: "db_mysql").
        consulta_sql (str): La consulta SQL (SELECT) para extraer los datos.
        archivo_csv (str): El nombre del archivo CSV a sobreescribir.

    Returns:
        bool: True si la operaci√≥n fue exitosa, False si hubo un error.
    """

    try:
        # NUEVO: Verificar que exista la configuraci√≥n en secrets
        if nombre_conexion_st not in st.secrets.get("connections", {}):
            st.error(f"‚ùå No se encontr√≥ '[connections.{nombre_conexion_st}]' en secrets.toml")
            st.info("""
                    **Soluci√≥n:** Crea el archivo `.streamlit/secrets.toml` con:
        ```toml
                    [connections.db_mysql]
                    dialect = "mysql"
                    driver = "mysqlconnector"
                    host = "localhost"
                    port = 3306
                    database = "tu_base_datos"
                    username = "tu_usuario"
                    password = "tu_contrase√±a"
        ```
                    """)
            return False

        else:
            # 1. Establecer conexi√≥n usando el gestor de Streamlit
            # Streamlit buscar√° en secrets.toml la secci√≥n [connections.nombre_conexion_st]
            st.info(f"Conectando a DB MySQL", icon="‚úÖ")
            conn = st.connection(nombre_conexion_st, type="sql")

            # 2. Ejecutar la consulta.
            # Usamos ttl=0 para deshabilitar el cacheo y asegurar que los datos
            # sean siempre los m√°s recientes (vital para una "actualizaci√≥n").
            # Si quisiera cachear los resultados por 10 minutos, usar√≠a ttl=600.
            df_datos = conn.query(consulta_sql, ttl=0)

            # 3. Guardar el DataFrame en el archivo CSV
            df_datos.to_csv(archivo_csv, index=False, encoding='utf-8')

            # 4. Notificar al usuario del √©xito
            st.info(f"¬°√âxito! el archivo de datos local ha sido actualizado correctamente.", icon="‚úÖ")
            return True

    except OperationalError as e:
        # Error com√∫n si la DB no responde, las credenciales son incorrectas
        # o la consulta SQL tiene un error de sintaxis o tabla no encontrada.
        st.error(f"Error de Conexi√≥n/SQL (OperationalError): {e}")
        return False

    except FileNotFoundError:
        # Error si st.connection no encuentra el driver (ej. 'mysqlconnector')
        st.error("Error: Driver de base de datos no encontrado.")
        st.warning("Aseg√∫rese de haber instalado 'mysql-connector-python'.")
        return False

    except Exception as e:
        # 8. Capturar cualquier otro error inesperado
        st.error(f"Error inesperado durante la actualizaci√≥n: {e}")
        return False


def get_last_update_time(archivo_csv: str) -> str:
    """
    Obtiene la fecha de √∫ltima modificaci√≥n de un archivo en formato legible.
    """
    try:
        # Obtener el timestamp (en segundos) de la √∫ltima modificaci√≥n
        timestamp = os.path.getmtime(archivo_csv)
        # Convertir el timestamp a un objeto datetime
        mod_time = datetime.fromtimestamp(timestamp)
        # Formatear la fecha para mostrarla
        return mod_time.strftime("%Y-%m-%d %H:%M:%S")
    except FileNotFoundError:
        return "Archivo no encontrado"
    except Exception as e:
        return f"Error al leer fecha: {e}"


@st.cache_data  # Usamos cache de Streamlit
def load_data(archivo_csv: str): #-> pd.DataFrame | None:
    """
    Carga y procesa el archivo CSV local.
    Se cachea para mejorar el rendimiento en interacciones (como filtros).
    La cach√© se limpiar√° manualmente si el bot√≥n de actualizar se pulsa.
    """
    try:
        df = pd.read_csv(archivo_csv)

        if df.empty:
            st.warning("El archivo CSV est√° vac√≠o.")
            return None
        # 1. Identificar filas donde 'region' est√° vac√≠a (NaN)
        # (usamos .isna() que detecta NaN, y .fillna('') por si acaso)
        df['region'] = df['region'].fillna(pd.NA)
        missing_region_mask = df['region'].isna()

        # 2. Comprobar si hay alguna regi√≥n que rellenar
        if missing_region_mask.any():

            # 3. Comprobar si la biblioteca 'reverse_geocoder' est√° disponible
            if rg is None:
                st.error("Biblioteca 'reverse_geocoder' no encontrada.")
                st.warning("Para rellenar regiones vac√≠as, instala la biblioteca: pip install reverse_geocoder")
                # Rellenar con 'Desconocida' para que el groupby funcione
                df['region'] = df['region'].fillna('Desconocida')
            else:
                st.info(f"Detectadas {missing_region_mask.sum()} regiones vac√≠as. Rellenado.")

                try:
                    # 4. Preparar las coordenadas (lat, lon) SOLO de las filas necesarias
                    coords = list(zip(
                        df.loc[missing_region_mask, 'lat'],
                        df.loc[missing_region_mask, 'lon']
                    ))

                    if coords:
                        # 5. Realizar la b√∫squeda (es offline y r√°pida)
                        results = rg.search(coords)  # Retorna una lista de dicts

                        # 6. Extraer el nombre de la regi√≥n ('admin1' en Espa√±a es la Comunidad Aut√≥noma)
                        # ej: 'Madrid', 'Andalusia', 'Catalonia'
                        filled_regions = [res['admin1'] for res in results]

                        # 7. Asignar los nuevos valores de vuelta al DataFrame
                        df.loc[missing_region_mask, 'region'] = filled_regions

                except Exception as e:
                    st.error(f"Error durante la geocodificaci√≥n: {e}")
                    df['region'] = df['region'].fillna('Error Geocodificaci√≥n')

        # Rellenar cualquier posible vac√≠o restante (ej. coordenadas en el oc√©ano)
        df['region'] = df['region'].fillna('Desconocida')
        
        # --- Procesamiento de Fechas ---
        # Convertir la columna de fecha/hora a objeto datetime de pandas
        df['fecha_hora_descarga'] = pd.to_datetime(df['fecha_hora_descarga'])

        # Crear una nueva columna 'fecha' (solo el d√≠a) para agrupar y filtrar
        df['fecha'] = df['fecha_hora_descarga'].dt.date

        return df

    except FileNotFoundError:
        # Esto es normal si es la primera vez que se ejecuta
        st.info(f"No se encontr√≥ el archivo '{archivo_csv}'. "
                "Pulsa el bot√≥n 'Actualizar' para generarlo desde la DB.")
        return None
    except pd.errors.EmptyDataError:
        st.warning(f"El archivo '{archivo_csv}' est√° vac√≠o.")
        return None
    except Exception as e:
        st.error(f"Error cr√≠tico al cargar o procesar el CSV: {e}")
        return None

# --- ESTRUCTURA PRINCIPAL DE LA APLICACI√ìN ---
import time
with st.sidebar:
    st.title("üóÇÔ∏è Panel de Control")

    # Definir la consulta y el nombre de la conexi√≥n
    MI_CONSULTA = "SELECT * FROM descargas;"
    MI_CONEXION_SECRETS = "db_mysql"  # Debe coincidir con [connections.db_mysql] en secrets.toml
    ARCHIVO_SALIDA = "data.csv"

    # --- 1. Bot√≥n de Actualizaci√≥n (L√≥gica existente) ---
    # Colocamos el bot√≥n primero
    if st.button(f"Actualizar Registros Base de Datos"):

        with st.spinner("Ejecutando consulta y actualizando CSV..."):

            exito = actualizar_csv_con_st_connection(
                nombre_conexion_st=MI_CONEXION_SECRETS,
                consulta_sql=MI_CONSULTA,
                archivo_csv=ARCHIVO_SALIDA)

            if exito:
             st.success("Proceso completado.")
             # ¬°IMPORTANTE!
             # Limpiamos la cach√© de la funci√≥n 'load_data'.
             # Esto fuerza a Streamlit a releer el archivo CSV
             # modificado en la siguiente l√≠nea (load_data).
             st.cache_data.clear()
            # 'else' (fallo) ya es manejado por la funci√≥n con st.error

    # --- 2. Carga y Visualizaci√≥n de Datos (NUEVA L√ìGICA) ---

    # Mostrar estado de carga y fecha de actualizaci√≥n (Metas 1 y 2)
    st.subheader("Estado de los Datos Locales")
    with st.spinner(f"Cargando datos en local."):
       # Intentamos cargar los datos (usar√° la cach√© si no se puls√≥ el bot√≥n)
      df_principal = load_data(ARCHIVO_SALIDA)

      # Obtenemos y mostramos la fecha de modificaci√≥n del archivo
      last_update = get_last_update_time(ARCHIVO_SALIDA)
      st.caption(f"√öltima actualizaci√≥n del fichero: **{last_update}**")

    # Si la carga falla (ej. no existe el archivo), detenemos la ejecuci√≥n aqu√≠
    if df_principal is None:
       st.stop()

# --- 3. An√°lisis y Visualizaciones (Metas 3, 4, 5, 6) ---

# Meta 4: Resumen por Zona


# Agrupar por 'zona'
df_zona = df_principal.groupby('region').agg(
    # Contar todos los registros (id) [cite: 1]
    numero_descargas=('id', 'count'),
    # Contar cu√°ntos 'id_descargado' √∫nicos hay [cite: 1]
    descargas_unicas=('id_descargado', 'nunique')
).reset_index()

df_zona.rename(columns={'region': 'Regi√≥n', 'numero_descargas':'Descargas', 'descargas_unicas':'Descargas √önicas'}, inplace=True)
# Calcular totales globales (antes de a√±adir la fila de total)
total_descargas_global = df_principal.shape[0]  # Total de filas
total_unicas_global = df_principal['id_descargado'].nunique()  # Total de IDs √∫nicos

st.title("üìà An√°lisis de Descargas | Certificados AyN")

col1, col2 = st.columns(2)
col1.markdown(":blue[Sistema de Seguimiento]")
col2.page_link("https://dedalogestion.com/ayn/", label="D√©dalo", icon="üü©")

st.markdown("""
<style>
    [data-testid="stMetricValue"] {
        font-size: 40px;
        font-weight: bold;  /* Hace el n√∫mero m√°s grueso */
        color: #023b61;     /* Color azul (opcional) */
    }

    /* Intenta varios selectores para la etiqueta */

    [data-testid="stMetric"] [data-testid="stMarkdownContainer"] {
        font-size: 20px !important;
        font-weight: bold !important;
    }
    }
</style>
""", unsafe_allow_html=True)


col1, col2 = st.columns(2)
col1.metric("**Total Descargas** Ô∏è", total_descargas_global, border=True)
col2.metric("**Total Descargas √önicas** ", total_unicas_global, border=True)
ficheros_totales = 15151
porcentaje = round((total_unicas_global/ficheros_totales)*100, 2)
st.progress(porcentaje/100)
st.text(f"Porcentaje de Ficheros √önicos Descargados: {porcentaje}%")
st.divider()

st.header("Resumen por Regi√≥n")
# Mostrar el DataFrame
st.dataframe(df_zona, hide_index='region', width= "content", use_container_width=True)

st.map(df_principal, color='#1b8210')

st.divider()

# Metas 5 y 6: An√°lisis por Fecha
st.header("An√°lisis por Fecha")
# Definir fechas m√≠nimas y m√°ximas para el selector (basado en la columna 'fecha')
min_date = df_principal['fecha'].min()
max_date = df_principal['fecha'].max()

# Meta 5 (parcial): Calendario para seleccionar rango
selected_range = st.date_input(
    "Seleccione un rango de fechas",
    (min_date, max_date),  # Valor por defecto (rango completo)
    min_value=min_date,
    max_value=max_date,
    key="date_range_selector")

# Asegurarse de que el selector devolvi√≥ un rango v√°lido (inicio y fin)
if len(selected_range) == 2:
    start_date, end_date = selected_range

    # Filtrar el DF principal seg√∫n el rango seleccionado
    df_filtrado_fecha = df_principal[
        (df_principal['fecha'] >= start_date) &
        (df_principal['fecha'] <= end_date)
        ]

    # Comprobar si hay datos en el rango seleccionado
    if df_filtrado_fecha.empty:
        st.warning("No hay datos en el rango de fechas seleccionado.")
    else:
        st.subheader("Histograma de Descargas Totales por D√≠a (en rango)")
        # Agregamos para el histograma (contando 'id' totales, no √∫nicos)
        df_hist = df_filtrado_fecha.groupby('fecha').agg(
            numero_descargas_totales=('id', 'count')).reset_index()

        # Pintar el histograma
        st.bar_chart(
            df_hist,
            x='fecha',
            y='numero_descargas_totales',
            x_label = 'Fecha',
            y_label = 'Numero de Descargas',
            use_container_width=True)

        st.subheader("Descargas √önicas por D√≠a (en rango)")
        df_fecha_agg = df_filtrado_fecha.groupby('fecha').agg(numero_descargas_unicas=('id_descargado', 'nunique')).reset_index()
        st.dataframe(df_fecha_agg, use_container_width=True)

else:
    # Caso por si el selector de fecha no devuelve un rango (ej. solo 1 d√≠a)
    st.info("Por favor, seleccione un rango de fechas v√°lido (inicio y fin).")

st.divider()